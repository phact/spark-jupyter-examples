{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some random data using mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.mllib.random.RandomRDDs._\n",
    "\n",
    "\n",
    "// Generate a random double RDD that contains 1 million i.i.d. values drawn from the\n",
    "// standard normal distribution `N(0, 1)`, evenly distributed in 10 partitions.\n",
    "val u = normalRDD(sc, 80000000L, 10)\n",
    "// Apply a transform to get a random double RDD following `N(1, 4)`.\n",
    "val v = u.map(x => 1.0 + 100000000.0 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Double = -479.4090689836632"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sum = u.take(1000000).reduce((x,y) => x+y)\n",
    "sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table using the DataStax driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.datastax.driver.core.ResultSet = ResultSet[ exhausted: true, Columns[]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.datastax.spark.connector._\n",
    "import com.datastax.spark.connector.cql.CassandraConnector\n",
    "import org.apache.spark.SparkConf\n",
    " \n",
    " \n",
    " val conf = new SparkConf(true)\n",
    " val connector = CassandraConnector(conf)\n",
    " connector.withSessionDo(session => {\n",
    "    session.execute(s\"create keyspace if not exists test with replication = { 'class':'SimpleStrategy',\"+\n",
    "          \" 'replication_factor':1}\")\n",
    "    session.execute(s\"create table if not exists test.test \" +\n",
    "        \"(id int, value int, PRIMARY KEY((id)))\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the random data to the cassandra table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "case class TestTableStructure(id: Double, value: Double)\n",
    "\n",
    "val data = v.map(x => TestTableStructure(x, 1))\n",
    "\n",
    "data.saveToCassandra(\"test\",\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val testData = sc.cassandraTable(\"test\",\"test\")\n",
    "val ids = testData.map(x => x.getDouble(\"id\"))\n",
    "ids.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a Kernel Density Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.stat.KernelDensity\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "\n",
    "// Construct the density estimator with the sample data and a standard deviation for the Gaussian\n",
    "// kernels\n",
    "val kd = new KernelDensity().setSample(ids).setBandwidth(.01)\n",
    "\n",
    "// Find density estimates for the given values\n",
    "val densities = kd.estimate(Array(1.0,10, 100, 1000,10000, 100000, 1000000, 10000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[Double] = Array(5.111711219173731E-7, 0.0, 0.0, 0.0, 5.111711219173731E-7, 0.0, 5.111711219173731E-7, 0.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark-DSE Cluster (Scala 2.10.4)",
   "language": "scala",
   "name": "spark-dse-cluster"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
